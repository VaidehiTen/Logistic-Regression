{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression | Assignment**"
      ],
      "metadata": {
        "id": "BTXJWYzcv7qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?**\n",
        "-  **Logistic Regression** and **Linear Regression** are both statistical and machine learning algorithms used for predictive modeling under the supervised learning paradigm. They both aim to uncover the relationship between `independent (predictor) variables` and a `dependent (outcome) variable`. However, a key distinction lies in the nature of the outcome variable and the problems they are best suited to solve.\n",
        "- **Logistic regression**\n",
        "  - Purpose: Primarily used for classification problems, where the goal is to predict a categorical outcome.\n",
        "  - Dependent Variable: Categorical, typically binary (e.g., yes/no, true/false, 0/1) but can extend to multiple unordered (multinomial) or ordered (ordinal) categories.\n",
        "  - Output: Predicts the probability that an input belongs to a specific class (a value between 0 and 1). A decision threshold (often 0.5) is then used to assign the input to a particular category.\n",
        "  - Mathematical Approach: Employs the logistic (sigmoid) function, an S-shaped curve, to transform a linear combination of inputs into a probability value between 0 and 1.\n",
        "  - Relationship between variables: Does not assume a linear relationship between the independent and dependent variables, but rather models the probability of the outcome.\n",
        "- **Linear regression**  \n",
        "  - Purpose: Used for regression problems, where the objective is to predict a continuous numerical value.\n",
        "  - Dependent Variable: Continuous (e.g., price, temperature, sales figures, test scores).\n",
        "  - Output: Predicts a specific numerical value of the dependent variable.\n",
        "  - Mathematical Approach: Uses a linear equation (Y = β0 + β1X + ε) to model the relationship between the independent and dependent variables.\n",
        "  - Relationship between variables: Assumes a linear relationship exists between the variables.\n",
        "-  Linear regression predicts a continuous numerical value (regression problems), while logistic regression predicts a categorical outcome (classification problems).\n",
        "- This difference is reflected in their mathematical approaches; linear regression uses a linear equation, and logistic regression uses the logistic (sigmoid) function to output probabilities.\n",
        "- While linear regression assumes a linear relationship between variables, logistic regression models the probability of an outcome without this assumption.  "
      ],
      "metadata": {
        "id": "oMvet1VwwAfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Explain the role of the Sigmoid function in Logistic Regression.**\n",
        "- The Sigmoid function, also known as the logistic function, is a fundamental component of Logistic Regression, a statistical model used for binary classification tasks. It plays a crucial role in converting the raw output of the model into a probability value between 0 and 1.This transformation is essential because probabilities must always be between 0 and 1, a range the sigmoid function is ideally suited to provide.\n",
        "\n",
        "  **Role of the Sigmoid function**\n",
        "\n",
        "- **Mapping to Probabilities**: In logistic regression, the goal is to predict the probability that a data point belongs to a specific class (e.g., whether an email is spam or not, or a tumor is malignant or not). However, the linear combination of features in a logistic regression model can produce any real-valued number, which is unsuitable for directly representing probabilities that are bounded between 0 and 1. The sigmoid function addresses this by transforming the linear output into a probability score within the valid range of 0 to 1.\n",
        "- **S-shaped Curve**: The sigmoid function produces an \"S\"-shaped curve, which is ideal for modeling the probability of a binary outcome. As the input to the sigmoid function becomes increasingly positive, the output approaches 1, indicating a high probability of belonging to the positive class. Conversely, as the input becomes increasingly negative, the output approaches 0, indicating a high probability of belonging to the negative class.\n",
        "- **Thresholding for Classification**: After the sigmoid function outputs a probability, a threshold (usually 0.5) is applied to classify the data point into one of the two classes. For example, if the probability is greater than or equal to 0.5, the data point is classified as belonging to class 1, otherwise it belongs to class 0.\n"
      ],
      "metadata": {
        "id": "bxt_4nsT_ACl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **What is Regularization in Logistic Regression and why is it needed?**\n",
        "- Regularization is a technique used in machine learning, including logistic regression, to prevent a common problem called overfitting.\n",
        "- Overfitting occurs when a model learns the training data too well, including the noise and random fluctuations, rather than just the underlying patterns. - This leads to excellent performance on the training data but poor performance on new data.\n",
        "- Imagine fitting a complex curve to perfectly match every point in a dataset, even the outliers. While this might fit the training data perfectly, it won't generalize well if new data points don't follow the exact same \"noise\" patterns.\n",
        "\n",
        " **Why is regularization needed?**\n",
        "\n",
        "- To prevent overfitting: As mentioned, logistic regression models can be prone to overfitting, especially with high-dimensional datasets or when the number of features exceeds the number of observations. Regularization helps to mitigate this issue, leading to a model that performs better on new, unseen data.\n",
        "- To handle multicollinearity: Multicollinearity occurs when independent variables are highly correlated with each other, which can lead to unstable and unreliable estimates of regression coefficients. Regularization, particularly L2 regularization (Ridge), can help manage this by reducing the impact of highly correlated features and distributing their influence more evenly.\n",
        "- To improve model interpretability (especially with L1 regularization): L1 regularization (Lasso) can drive the coefficients of less important features to zero, effectively performing feature selection and making the model easier to understand and interpret in terms of the most relevant features, according to Medium.\n",
        "- To improve model generalizability: By preventing overfitting and dealing with multicollinearity, regularization helps create models that generalize well to new data, leading to more reliable predictions in real-world scenarios.  "
      ],
      "metadata": {
        "id": "DflgrDtHCEud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **What are some common evaluation metrics for classification models, and\n",
        "why are they important?**\n",
        "- Classification models aim to categorize data into predefined labels or classes. Evaluating the performance of these models requires using appropriate metrics to understand their effectiveness and guide improvements.\n",
        "\n",
        " **Here are some common evaluation metrics for classification models and their importance:**\n",
        "\n",
        "- **Accuracy**\n",
        " - What it is: The proportion of correctly predicted instances out of the total instances.\n",
        " - Importance: A good initial measure for overall performance when classes are balanced and misclassification costs are roughly equal for all classes.\n",
        " - Limitations: Can be misleading with imbalanced datasets. For example, a model classifying 99% of data as the majority class could achieve high accuracy while failing to identify the minority class, according to GeeksforGeeks.\n",
        "- **Confusion matrix**\n",
        "  - What it is: A table that summarizes predictions versus actual class labels. It shows True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
        "  - Importance: It provides a clear picture of the model's accuracy and misclassifications, helping to identify error patterns.\n",
        "  - Components: This matrix includes:\n",
        "    - True Positives (TP): Correctly predicted positive instances.\n",
        "    - True Negatives (TN): Correctly predicted negative instances.\n",
        "    - False Positives (FP) (Type I Error): Incorrectly predicted positive instances (actual negative).\n",
        "    - False Negatives (FN) (Type II Error): Incorrectly predicted negative instances (actual positive).\n",
        "- **Precision**\n",
        "  - What it is: The ratio of true positives to the total number of positive predictions. It measures how many positively predicted instances were actually positive.\n",
        "  - Importance: Precision is valuable when minimizing false positives is critical, such as in spam detection or financial fraud. High precision indicates a low rate of false alarms.\n",
        "- **Recall (Sensitivity, True Positive Rate)**\n",
        "-  - What it is: The ratio of true positives to the total number of actual positive instances. It indicates how many of the actual positive cases were correctly identified by the model.\n",
        "  - Importance: Recall is crucial when missing true positives has significant consequences, such as in disease detection or security threat identification.\n",
        "- **F1 score**\n",
        "  - What it is: The harmonic mean of precision and recall.\n",
        "  - Importance: It offers a balanced measure of performance, especially when there's a trade-off between precision and recall, or with imbalanced datasets. It helps to balance the impact of false positives and false negatives.\n",
        "- **AUC-ROC Curve (Receiver Operating Characteristic Area Under the Curve)**\n",
        "  - What it is: Measures the model's ability to distinguish between positive and negative classes across various thresholds. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR), and the AUC is the area under this curve.\n",
        "  - Importance: It provides insight into the model's discrimination power, particularly for binary classification problems. A higher AUC indicates better class separation.\n",
        "- **Logarithmic Loss (Log Loss)**\n",
        "  - What it is: Evaluates the uncertainty of a model's predictions based on the probability assigned to predicted classes.\n",
        "  - Importance: Useful for assessing the quality of predicted probabilities, especially when prediction confidence is important or with imbalanced datasets. Lower log loss suggests more accurate probability estimates.\n",
        "\n",
        "  **Why these metrics are important :**\n",
        "\n",
        "- Assessing Performance: They provide a way to quantitatively measure how well a model performs.\n",
        "- Informing Decisions: Understanding these metrics helps determine if a model is suitable for deployment or requires further development.\n",
        "- Identifying Strengths and Weaknesses: Each metric offers a different perspective on the model's behavior.\n",
        "- Optimizing Performance: Analyzing metrics allows for iterative refinement to improve the model and align it with specific goals.\n",
        "- Choosing the Right Metric: Different problems require different priorities. For example, a medical diagnosis model would prioritize recall, while a spam filter might prioritize precision. Selecting the appropriate metric ensures the model's performance aligns with the problem's context.   "
      ],
      "metadata": {
        "id": "msG-KqGOEz1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)**\n"
      ],
      "metadata": {
        "id": "Ier6sonXHtYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Display first 5 rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)  # Increased max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TsDiQbFIEEm",
        "outputId": "42eb0e92-0329-4ebf-b3f8-abda5199d580"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the dataset:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)**\n"
      ],
      "metadata": {
        "id": "wVNQfFxfIxfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, multi_class='auto')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print coefficients and accuracy\n",
        "print(\"Model Coefficients (per class):\")\n",
        "print(model.coef_)\n",
        "print(\"\\nIntercepts (per class):\")\n",
        "print(model.intercept_)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irup1g0II4ni",
        "outputId": "01675016-64af-462a-beb9-3a1fe7bd475c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients (per class):\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n",
            "\n",
            "Intercepts (per class):\n",
            "[  9.00884295   1.86902164 -10.87786459]\n",
            "\n",
            "Model Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "QV9O7GagJO8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with one-vs-rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwMDU8DxJVxU",
        "outputId": "ddf43977-8f07-4a69-c0de-f67db73804ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "aTcIJp5nJ5hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Base logistic regression model\n",
        "base_model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "\n",
        "# Wrap in One-vs-Rest classifier\n",
        "ovr_model = OneVsRestClassifier(base_model)\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'estimator__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'estimator__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(ovr_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid.best_score_:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RoOYid-K2XI",
        "outputId": "e97fa3a2-be4f-4fbd-abfd-3b6ca04a2135"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'estimator__C': 10, 'estimator__penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "Z0oqH_8mLDnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---------------- Without Scaling ----------------\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---------------- With Scaling ----------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(max_iter=200)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# ---------------- Results ----------------\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with scaling   : {acc_scaling:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq3KjAd8LIpT",
        "outputId": "ea389afd-9a14-478c-947f-a9b7077f3b2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.00\n",
            "Accuracy with scaling   : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**\n",
        "\n",
        " **Marketing Campaign Response Prediction — Logistic Regression Plan**\n",
        "\n",
        "- Problem Context\n",
        "  - Business goal: Identify customers most likely to respond to a campaign.\n",
        "  - Dataset: Highly imbalanced (5% responders, 95% non-responders).\n",
        "  - Challenge: Standard accuracy will be misleading — we need a method that prioritizes finding responders while controlling costs.\n",
        "- Data Handling\n",
        "  - Data Cleaning\n",
        "    - Remove duplicates.\n",
        "    - Handle missing values (imputation for numeric/categorical fields).\n",
        "    - Standardize data formats (e.g., date fields to datetime).\n",
        "  - Feature Engineering\n",
        "    - Customer activity features (recent purchases, website visits).\n",
        "    - Engagement history (previous campaign responses).\n",
        "    - Demographic features (location, age group, income bracket).\n",
        "  - Encoding\n",
        "    - One-hot encoding for categorical variables.\n",
        "    - Avoid dummy variable trap (drop one category).\n",
        "  - Feature Selection\n",
        "    - Remove highly correlated features to avoid multicollinearity.\n",
        "    - Use domain knowledge + statistical tests.  \n",
        "- Feature Scaling\n",
        "  - Logistic Regression uses distance-based optimization — scaling improves convergence.\n",
        "  - Apply StandardScaler to all numeric features.\n",
        "  - Scaling applied after train-test split to avoid data leakage.\n",
        "- Addressing Class Imbalance\n",
        "  - Class Weights Approach: Set `class_weight='balanced'` so minority class gets proportionally higher weight in loss function.\n",
        "  - Resampling Approach:\n",
        "    - Oversample minority class with SMOTE.\n",
        "    - Optionally undersample majority class for computational efficiency.\n",
        "  - Why: Ensures the model doesn’t just predict \"non-response\" for everyone.\n",
        "- Model Training & Hyperparameter Tuning\n",
        "  - Base model:\n",
        "\n",
        "     ```\n",
        "     # LogisticRegression(solver='liblinear', max_iter=500)\n",
        "     ```\n",
        "\n",
        "  - Hyperparameters to tune:\n",
        "    - C (inverse regularization strength): [0.01, 0.1, 1, 10]\n",
        "    - penalty: ['l1', 'l2']\n",
        "    - class_weight: ['balanced', None]\n",
        "  - Tuning method:\n",
        "    - Use GridSearchCV with StratifiedKFold (preserves class ratio in folds).\n",
        "    - Optimize for F1-score (balances precision and recall).      \n",
        "- Model Evaluation\n",
        "  - Confusion Matrix: Understand FN (missed responders) and FP (extra campaign costs).\n",
        "  - Metrics:\n",
        "    - Precision: Of those predicted to respond, how many did.\n",
        "    - Recall: Of actual responders, how many were found.\n",
        "    - F1-score: Balance between precision and recall.\n",
        "    - ROC-AUC: Class separation ability.\n",
        "    - PR-AUC: Better for imbalanced problems.\n",
        "  - Threshold tuning:\n",
        "    - Default decision threshold is 0.5.\n",
        "    - Adjust based on business priorities (e.g., set to 0.3 to increase recall).\n",
        "- Deployment Plan\n",
        "  - Integrate model into campaign management system.\n",
        "  - Output a ranked list of customers with probability scores.\n",
        "  - Let marketing choose a cutoff based on budget and response rate goals.\n",
        "- Monitoring\n",
        "  - Track actual campaign performance vs predicted probabilities.\n",
        "  - Watch for data drift (e.g., change in purchase behavior over time).\n",
        "  - Schedule retraining every quarter or after major market changes."
      ],
      "metadata": {
        "id": "XA28yvstLcO0"
      }
    }
  ]
}